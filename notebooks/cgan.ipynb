{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# paths to the dataset\n",
    "root_dir = '../data/IAM'\n",
    "forms_file = os.path.join(root_dir, 'ascii/forms.txt')\n",
    "images_dir = os.path.join(root_dir, 'forms/formsA-D')\n",
    "\n",
    "# parse 'forms.txt' to extract metadata\n",
    "def parse_forms_file(forms_file):\n",
    "    with open(forms_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        fields = line.strip().split()\n",
    "        img_file, writer_id, text = fields[0], fields[1], ' '.join(fields[2:])\n",
    "        data.append({'image': img_file + '.png', 'writer': writer_id, 'text': text})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# create a filtered dataset for a single writer\n",
    "def filter_dataset_by_writer(dataframe, writer_id):\n",
    "    return dataframe[dataframe['writer'] == writer_id]\n",
    "\n",
    "# parse and filter dataset\n",
    "data = parse_forms_file(forms_file)\n",
    "writer_id = data['writer'].iloc[0]\n",
    "filtered_data = filter_dataset_by_writer(data, writer_id=writer_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, dataframe, images_dir, transform=None, max_text_len=50):\n",
    "        self.dataframe = dataframe\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.max_text_len = max_text_len\n",
    "        \n",
    "        # Create character vocabulary from the dataset\n",
    "        self.chars = set()\n",
    "        for text in dataframe['text']:\n",
    "            self.chars.update(set(text))\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(sorted(self.chars))}\n",
    "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
    "        self.vocab_size = len(self.char_to_idx)\n",
    "\n",
    "    def text_to_tensor(self, text):\n",
    "        # Convert text to one-hot encoded tensor\n",
    "        indices = [self.char_to_idx.get(c, 0) for c in text[:self.max_text_len]]\n",
    "        # Pad if necessary\n",
    "        indices = indices + [0] * (self.max_text_len - len(indices))\n",
    "        return torch.tensor(indices, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        img_path = os.path.join(self.images_dir, row['image'])\n",
    "        img = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "        \n",
    "        # Center crop to maintain aspect ratio\n",
    "        w, h = img.size\n",
    "        min_dim = min(w, h)\n",
    "        img = transforms.CenterCrop(min_dim)(img)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            \n",
    "        text_tensor = self.text_to_tensor(row['text'])\n",
    "        return img, text_tensor, row['writer']\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])  # normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "dataset = IAMDataset(filtered_data, images_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, text_dim, style_dim, channels=64):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.text_embedding = nn.Embedding(text_dim, 256)  # Text embedding layer\n",
    "        self.style_embedding = nn.Linear(style_dim, 256)   # Style embedding layer\n",
    "        \n",
    "        # Initial dense layer\n",
    "        self.fc = nn.Linear(noise_dim + 256 + 256, 4 * 4 * channels * 8)\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # 4x4 -> 8x8\n",
    "            nn.ConvTranspose2d(channels * 8, channels * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(channels * 4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 8x8 -> 16x16\n",
    "            nn.ConvTranspose2d(channels * 4, channels * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(channels * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 16x16 -> 32x32\n",
    "            nn.ConvTranspose2d(channels * 2, channels, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # 32x32 -> 64x64\n",
    "            nn.ConvTranspose2d(channels, 1, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, text, style):\n",
    "        # Embed text and style\n",
    "        text_embedding = self.text_embedding(text).mean(dim=1)  # Average over sequence length\n",
    "        style_embedding = self.style_embedding(style)\n",
    "        \n",
    "        # Concatenate inputs\n",
    "        x = torch.cat([noise, text_embedding, style_embedding], dim=1)\n",
    "        \n",
    "        # Generate image\n",
    "        x = self.fc(x)\n",
    "        x = x.view(x.size(0), -1, 4, 4)\n",
    "        x = self.conv_blocks(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, text_dim, style_dim, channels=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.text_embedding = nn.Embedding(text_dim, 256)\n",
    "        self.style_embedding = nn.Linear(style_dim, 256)\n",
    "        \n",
    "        # Image processing\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            # 64x64 -> 32x32\n",
    "            nn.Conv2d(1, channels, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 32x32 -> 16x16\n",
    "            nn.Conv2d(channels, channels * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(channels * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 16x16 -> 8x8\n",
    "            nn.Conv2d(channels * 2, channels * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(channels * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # 8x8 -> 4x4\n",
    "            nn.Conv2d(channels * 4, channels * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(channels * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Final classification layer\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(channels * 8 * 4 * 4 + 256 + 256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, text, style):\n",
    "        # Process image\n",
    "        img_features = self.conv_blocks(img)\n",
    "        img_features = img_features.view(img_features.size(0), -1)\n",
    "        \n",
    "        # Process text and style\n",
    "        text_embedding = self.text_embedding(text).mean(dim=1)\n",
    "        style_embedding = self.style_embedding(style)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        x = torch.cat([img_features, text_embedding, style_embedding], dim=1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/100] [Batch 0/2] [D loss: 0.7405] [G loss: 0.6848]\n",
      "[Epoch 1/100] [Batch 0/2] [D loss: 0.0321] [G loss: 2.8305]\n",
      "[Epoch 2/100] [Batch 0/2] [D loss: 0.0312] [G loss: 2.8433]\n",
      "[Epoch 3/100] [Batch 0/2] [D loss: 0.0148] [G loss: 3.6401]\n",
      "[Epoch 4/100] [Batch 0/2] [D loss: 0.0069] [G loss: 4.6671]\n",
      "[Epoch 5/100] [Batch 0/2] [D loss: 0.1548] [G loss: 1.3468]\n",
      "[Epoch 6/100] [Batch 0/2] [D loss: 0.0067] [G loss: 4.8182]\n",
      "[Epoch 7/100] [Batch 0/2] [D loss: 0.0042] [G loss: 5.5506]\n",
      "[Epoch 8/100] [Batch 0/2] [D loss: 0.0315] [G loss: 2.8618]\n",
      "[Epoch 9/100] [Batch 0/2] [D loss: 0.0147] [G loss: 3.6471]\n",
      "[Epoch 10/100] [Batch 0/2] [D loss: 0.0229] [G loss: 3.1662]\n",
      "[Epoch 11/100] [Batch 0/2] [D loss: 0.0120] [G loss: 3.8659]\n",
      "[Epoch 12/100] [Batch 0/2] [D loss: 0.0156] [G loss: 3.5671]\n",
      "[Epoch 13/100] [Batch 0/2] [D loss: 0.0119] [G loss: 3.8232]\n",
      "[Epoch 14/100] [Batch 0/2] [D loss: 0.0100] [G loss: 4.0199]\n",
      "[Epoch 15/100] [Batch 0/2] [D loss: 0.0094] [G loss: 4.0849]\n",
      "[Epoch 16/100] [Batch 0/2] [D loss: 0.0094] [G loss: 4.0572]\n",
      "[Epoch 17/100] [Batch 0/2] [D loss: 0.0085] [G loss: 4.1903]\n",
      "[Epoch 18/100] [Batch 0/2] [D loss: 0.0074] [G loss: 4.3187]\n",
      "[Epoch 19/100] [Batch 0/2] [D loss: 0.0066] [G loss: 4.4183]\n",
      "[Epoch 20/100] [Batch 0/2] [D loss: 0.0070] [G loss: 4.4111]\n",
      "[Epoch 21/100] [Batch 0/2] [D loss: 0.0067] [G loss: 4.4137]\n",
      "[Epoch 22/100] [Batch 0/2] [D loss: 0.0067] [G loss: 4.4060]\n",
      "[Epoch 23/100] [Batch 0/2] [D loss: 0.0071] [G loss: 4.3425]\n",
      "[Epoch 24/100] [Batch 0/2] [D loss: 0.0048] [G loss: 4.7796]\n",
      "[Epoch 25/100] [Batch 0/2] [D loss: 0.0045] [G loss: 4.8522]\n",
      "[Epoch 26/100] [Batch 0/2] [D loss: 0.0048] [G loss: 4.7513]\n",
      "[Epoch 27/100] [Batch 0/2] [D loss: 0.0047] [G loss: 4.7962]\n",
      "[Epoch 28/100] [Batch 0/2] [D loss: 0.0040] [G loss: 4.9227]\n",
      "[Epoch 29/100] [Batch 0/2] [D loss: 0.0036] [G loss: 5.0346]\n",
      "[Epoch 30/100] [Batch 0/2] [D loss: 0.0037] [G loss: 5.0090]\n",
      "[Epoch 31/100] [Batch 0/2] [D loss: 0.0033] [G loss: 5.1085]\n",
      "[Epoch 32/100] [Batch 0/2] [D loss: 0.0032] [G loss: 5.1489]\n",
      "[Epoch 33/100] [Batch 0/2] [D loss: 0.0035] [G loss: 5.0284]\n",
      "[Epoch 34/100] [Batch 0/2] [D loss: 0.0032] [G loss: 5.1561]\n",
      "[Epoch 35/100] [Batch 0/2] [D loss: 0.0031] [G loss: 5.2105]\n",
      "[Epoch 36/100] [Batch 0/2] [D loss: 0.0040] [G loss: 4.9075]\n",
      "[Epoch 37/100] [Batch 0/2] [D loss: 0.0032] [G loss: 5.1838]\n",
      "[Epoch 38/100] [Batch 0/2] [D loss: 0.0028] [G loss: 5.3062]\n",
      "[Epoch 39/100] [Batch 0/2] [D loss: 0.0028] [G loss: 5.2697]\n",
      "[Epoch 40/100] [Batch 0/2] [D loss: 0.0026] [G loss: 5.3803]\n",
      "[Epoch 41/100] [Batch 0/2] [D loss: 0.0026] [G loss: 5.3299]\n",
      "[Epoch 42/100] [Batch 0/2] [D loss: 0.0029] [G loss: 5.2982]\n",
      "[Epoch 43/100] [Batch 0/2] [D loss: 0.0028] [G loss: 5.2860]\n",
      "[Epoch 44/100] [Batch 0/2] [D loss: 0.0021] [G loss: 5.5977]\n",
      "[Epoch 45/100] [Batch 0/2] [D loss: 0.0017] [G loss: 5.8422]\n",
      "[Epoch 46/100] [Batch 0/2] [D loss: 0.0022] [G loss: 5.5641]\n",
      "[Epoch 47/100] [Batch 0/2] [D loss: 0.0024] [G loss: 5.4517]\n",
      "[Epoch 48/100] [Batch 0/2] [D loss: 0.0019] [G loss: 5.7348]\n",
      "[Epoch 49/100] [Batch 0/2] [D loss: 0.0018] [G loss: 5.7514]\n",
      "[Epoch 50/100] [Batch 0/2] [D loss: 0.0018] [G loss: 5.7395]\n",
      "[Epoch 51/100] [Batch 0/2] [D loss: 0.0019] [G loss: 5.7024]\n",
      "[Epoch 52/100] [Batch 0/2] [D loss: 0.0018] [G loss: 5.7534]\n",
      "[Epoch 53/100] [Batch 0/2] [D loss: 0.0018] [G loss: 5.7797]\n",
      "[Epoch 54/100] [Batch 0/2] [D loss: 0.0017] [G loss: 5.8809]\n",
      "[Epoch 55/100] [Batch 0/2] [D loss: 0.0015] [G loss: 5.9578]\n",
      "[Epoch 56/100] [Batch 0/2] [D loss: 0.0017] [G loss: 5.8190]\n",
      "[Epoch 57/100] [Batch 0/2] [D loss: 0.0019] [G loss: 5.6437]\n",
      "[Epoch 58/100] [Batch 0/2] [D loss: 0.0014] [G loss: 6.0300]\n",
      "[Epoch 59/100] [Batch 0/2] [D loss: 0.0016] [G loss: 5.9241]\n",
      "[Epoch 60/100] [Batch 0/2] [D loss: 0.0019] [G loss: 5.6903]\n",
      "[Epoch 61/100] [Batch 0/2] [D loss: 0.0017] [G loss: 5.8021]\n",
      "[Epoch 62/100] [Batch 0/2] [D loss: 0.0018] [G loss: 5.7616]\n",
      "[Epoch 63/100] [Batch 0/2] [D loss: 0.0020] [G loss: 5.6452]\n",
      "[Epoch 64/100] [Batch 0/2] [D loss: 0.0015] [G loss: 5.9813]\n",
      "[Epoch 65/100] [Batch 0/2] [D loss: 0.0016] [G loss: 5.9005]\n",
      "[Epoch 66/100] [Batch 0/2] [D loss: 0.0012] [G loss: 6.2599]\n",
      "[Epoch 67/100] [Batch 0/2] [D loss: 0.0015] [G loss: 5.9952]\n",
      "[Epoch 68/100] [Batch 0/2] [D loss: 0.0013] [G loss: 6.1070]\n",
      "[Epoch 69/100] [Batch 0/2] [D loss: 0.0017] [G loss: 5.8516]\n",
      "[Epoch 70/100] [Batch 0/2] [D loss: 0.0015] [G loss: 6.0258]\n",
      "[Epoch 71/100] [Batch 0/2] [D loss: 0.0014] [G loss: 6.2141]\n",
      "[Epoch 72/100] [Batch 0/2] [D loss: 0.0017] [G loss: 5.8777]\n",
      "[Epoch 73/100] [Batch 0/2] [D loss: 0.0014] [G loss: 6.1002]\n",
      "[Epoch 74/100] [Batch 0/2] [D loss: 0.0018] [G loss: 5.8395]\n",
      "[Epoch 75/100] [Batch 0/2] [D loss: 0.0015] [G loss: 6.0627]\n",
      "[Epoch 76/100] [Batch 0/2] [D loss: 0.0016] [G loss: 5.9177]\n",
      "[Epoch 77/100] [Batch 0/2] [D loss: 0.0016] [G loss: 6.1055]\n",
      "[Epoch 78/100] [Batch 0/2] [D loss: 0.0013] [G loss: 6.3135]\n",
      "[Epoch 79/100] [Batch 0/2] [D loss: 0.0014] [G loss: 6.1119]\n",
      "[Epoch 80/100] [Batch 0/2] [D loss: 0.0013] [G loss: 6.1457]\n",
      "[Epoch 81/100] [Batch 0/2] [D loss: 0.0017] [G loss: 6.0321]\n",
      "[Epoch 82/100] [Batch 0/2] [D loss: 0.0014] [G loss: 6.1850]\n",
      "[Epoch 83/100] [Batch 0/2] [D loss: 0.0017] [G loss: 5.9646]\n",
      "[Epoch 84/100] [Batch 0/2] [D loss: 0.0019] [G loss: 5.9293]\n",
      "[Epoch 85/100] [Batch 0/2] [D loss: 0.0019] [G loss: 6.0126]\n",
      "[Epoch 86/100] [Batch 0/2] [D loss: 0.0024] [G loss: 5.5656]\n",
      "[Epoch 87/100] [Batch 0/2] [D loss: 0.0017] [G loss: 7.6006]\n",
      "[Epoch 88/100] [Batch 0/2] [D loss: 0.0022] [G loss: 5.6525]\n",
      "[Epoch 89/100] [Batch 0/2] [D loss: 0.0014] [G loss: 7.1660]\n",
      "[Epoch 90/100] [Batch 0/2] [D loss: 0.0011] [G loss: 8.0192]\n",
      "[Epoch 91/100] [Batch 0/2] [D loss: 0.0006] [G loss: 7.5058]\n",
      "[Epoch 92/100] [Batch 0/2] [D loss: 0.0007] [G loss: 7.4305]\n",
      "[Epoch 93/100] [Batch 0/2] [D loss: 0.0007] [G loss: 7.6351]\n",
      "[Epoch 94/100] [Batch 0/2] [D loss: 0.0521] [G loss: 2.3387]\n",
      "[Epoch 95/100] [Batch 0/2] [D loss: 4.3400] [G loss: 12.0715]\n",
      "[Epoch 96/100] [Batch 0/2] [D loss: 0.7302] [G loss: 14.0057]\n",
      "[Epoch 97/100] [Batch 0/2] [D loss: 0.0221] [G loss: 4.5600]\n",
      "[Epoch 98/100] [Batch 0/2] [D loss: 0.0986] [G loss: 11.5020]\n",
      "[Epoch 99/100] [Batch 0/2] [D loss: 0.0389] [G loss: 7.8743]\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "lr = 0.0002\n",
    "b1 = 0.5\n",
    "b2 = 0.999\n",
    "noise_dim = 100\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(noise_dim=noise_dim, \n",
    "                     text_dim=dataset.vocab_size,\n",
    "                     style_dim=10).to(device)\n",
    "discriminator = Discriminator(text_dim=dataset.vocab_size,\n",
    "                            style_dim=10).to(device)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Loss functions\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (real_imgs, texts, style_ids) in enumerate(dataloader):\n",
    "        batch_size = real_imgs.size(0)\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        texts = texts.to(device)\n",
    "        \n",
    "        # Convert style_ids to one-hot vectors\n",
    "        style_emb = F.one_hot(torch.tensor([int(s) for s in style_ids]), num_classes=10).float().to(device)\n",
    "        \n",
    "        # Ground truths\n",
    "        valid = torch.ones(batch_size, 1).to(device)\n",
    "        fake = torch.zeros(batch_size, 1).to(device)\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Generate images\n",
    "        z = torch.randn(batch_size, noise_dim).to(device)\n",
    "        gen_imgs = generator(z, texts, style_emb)\n",
    "        \n",
    "        # Calculate loss\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs, texts, style_emb), valid)\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Real images\n",
    "        real_loss = adversarial_loss(discriminator(real_imgs, texts, style_emb), valid)\n",
    "        \n",
    "        # Fake images\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach(), texts, style_emb), fake)\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        \n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(\n",
    "                f\"[Epoch {epoch}/{num_epochs}] \"\n",
    "                f\"[Batch {i}/{len(dataloader)}] \"\n",
    "                f\"[D loss: {d_loss.item():.4f}] \"\n",
    "                f\"[G loss: {g_loss.item():.4f}]\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_handwriting(generator, text, style_id, dataset, device):\n",
    "    generator.eval()\n",
    "    \n",
    "    # Convert text to tensor using dataset's vocabulary\n",
    "    text_tensor = dataset.text_to_tensor(text).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Create style embedding\n",
    "    style_emb = F.one_hot(torch.tensor([style_id]), num_classes=10).float().to(device)\n",
    "    \n",
    "    # Generate image\n",
    "    z = torch.randn(1, noise_dim).to(device)\n",
    "    with torch.no_grad():\n",
    "        gen_img = generator(z, text_tensor, style_emb)\n",
    "        \n",
    "    # Convert to PIL image\n",
    "    gen_img = gen_img.squeeze().cpu()\n",
    "    gen_img = (gen_img + 1) / 2  # Denormalize from [-1, 1] to [0, 1]\n",
    "    return transforms.ToPILImage()(gen_img)\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"Hello World\"\n",
    "style_id = 1\n",
    "generated_image = generate_handwriting(generator, sample_text, style_id, dataset, device)\n",
    "generated_image.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
